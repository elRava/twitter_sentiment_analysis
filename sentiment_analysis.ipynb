{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data given is in the form of a comma-separated values files with tweets and their corresponding sentiments. The training dataset is a csv file of type tweet_id,sentiment,tweet where the\n",
    "tweet_id is a unique integer identifying the tweet, sentiment is either 1 (positive) or 0 (negative), and tweet is the tweet enclosed in \"\". Similarly, the test dataset is a csv file of type\n",
    "tweet_id,tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.insert(0, 'src')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/train.csv', encoding = \"ISO-8859-1\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # remove punctation\n",
    "    word = word.strip('\\'\"?!,.():;*')\n",
    "    # more than 3 letter repetition removed\n",
    "    word = re.sub(r'(.)\\1\\1+', r'\\1\\1\\1', word)\n",
    "    # remove - & '\n",
    "    word = word.strip('-&\\'')\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "def preprocess_tweet(tweet, use_stemmer=False, use_lemmatizer=False):\n",
    "    # convert tweet to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # replace urls with 'URL'\n",
    "    tweet = re.sub(r'((www.[\\S]+)|(https?://.[\\S]+))', 'URL', tweet)\n",
    "    # replace user mentions @user with 'USER_MENTION'\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # replace #hashtag with hastag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1', tweet)\n",
    "    # remove retweet RT\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # remove space, \" and ' \n",
    "    tweet.strip('\" \\'')\n",
    "    # handle emojis. Use only EMO_POS and EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # replace multiple spaces with only one space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    # preprocess words\n",
    "    words = tweet.split()\n",
    "\n",
    "    processed_words = []\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            if use_stemmer:\n",
    "                # use stemmer\n",
    "                word = str(porter_stemmer.stem(word))\n",
    "            elif use_lemmatizer:\n",
    "                word = str(wordnet_lemmatizer.lemmatize(word))\n",
    "            processed_words.append(word)\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:                      is so sad for my APL friend.............\n",
      "STEMMER: is so sad for my apl friend\n",
      "LEMMATIZER: is so sad for my apl friend\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:                    I missed the New Moon trailer...\n",
      "STEMMER: i miss the new moon trailer\n",
      "LEMMATIZER: i missed the new moon trailer\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:               omg its already 7:30 :O\n",
      "STEMMER: omg it alreadi o\n",
      "LEMMATIZER: omg it already o\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:           .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
      "STEMMER: omgaga im sooo im gunna cri been at thi dentist sinc i wa supos just get a crown put on\n",
      "LEMMATIZER: omgaga im sooo im gunna cry been at this dentist since i wa suposed just get a crown put on\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#tweet = \"Hi, how is going on? :) CIAO COME vaaaaa? :) :( https://tartarus.org/martin/PorterStemmer/  bella @ivocerti    #sentimentanalysis\"\n",
    "for i in range(4):\n",
    "    tweet = dataset.loc[i]['SentimentText']\n",
    "    print('ORIGINAL: ' + tweet)\n",
    "    print('STEMMER: ' + preprocess_tweet(tweet, use_stemmer=True))\n",
    "    print('LEMMATIZER: ' + preprocess_tweet(tweet, use_lemmatizer=True))\n",
    "    print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preprocessed_train_file = open('dataset/preprocessed_train.csv', 'w')\n",
    "preprocessed_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "for index, row in dataset.iterrows():\n",
    "    res = preprocess_tweet(row['SentimentText'])\n",
    "    preprocessed_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + preprocess_tweet(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry been at this denti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                        is so sad for my apl friend\n",
       "1       2          0                      i missed the new moon trailer\n",
       "2       3          1                                  omg its already o\n",
       "3       4          0  omgaga im sooo im gunna cry been at this denti...\n",
       "4       5          0                i think mi bf is cheating on me t_t"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset = pd.read_csv('dataset/preprocessed_train.csv', encoding = \"ISO-8859-1\")\n",
    "preprocessed_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature parameters (Word, SentiWordNet, POS + POS_Word, Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def word_to_SWNt(word):\n",
    "    # take the first result from sentiwordnet (if there is)\n",
    "    list_sent = list(swn.senti_synsets(word))\n",
    "    if len(list_sent) > 0:\n",
    "        sent = list_sent[0]\n",
    "        pos = round(sent.pos_score()*10)\n",
    "        neg = round(sent.neg_score()*10)\n",
    "        obj = round(sent.obj_score()*10)\n",
    "        # return the word if the word itself is mostly object\n",
    "        # otherwise return POS-X or NEG-X if the word is \n",
    "        # mostly positive or negative and X is the rounded score*10 (from 0 to 10)\n",
    "        if pos > neg:\n",
    "            return 'POS-' + str(pos)\n",
    "        elif neg > pos:\n",
    "            return 'NEG-' + str(neg)\n",
    "    return word\n",
    "\n",
    "def tweet_to_feature_word(tweet):\n",
    "    # just return the tweet as it is\n",
    "    return str(tweet)\n",
    "\n",
    "def tweet_to_feature_SWNt(tweet):\n",
    "    words = str(tweet).split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        processed_words.append(word_to_SWNt(word))\n",
    "    return str(' '.join(processed_words))\n",
    "\n",
    "def tweet_to_feature_POS(tweet):\n",
    "    # the tweet is composed by the POS (parts of speech) of the words\n",
    "    # first just the list of pos, then the list of the pos and the words\n",
    "    # in the format pos_word\n",
    "    words = str(tweet).split()\n",
    "    words_POS = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    pos = []\n",
    "    for word in words_POS:\n",
    "        pos.append(word[1])\n",
    "    processed_words = pos\n",
    "    size = len(processed_words)\n",
    "    for i in range(size):\n",
    "        processed_words.append(processed_words[i] + '_' + words[i])\n",
    "    return str(' '.join(processed_words))\n",
    "\n",
    "def tweet_to_feature_semantic(tweet):\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: is so sad for my apl friend\n",
      "SWNt: POS-2 so NEG-8 for my apl POS-1\n",
      "POS+POS_word: VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN_for PRP$_my NN_apl NN_friend\n",
      "----------------------------------------------------------------\n",
      "WORD: i missed the new moon trailer\n",
      "SWNt: i NEG-2 the POS-4 moon trailer\n",
      "POS+POS_word: NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_new NN_moon NN_trailer\n",
      "----------------------------------------------------------------\n",
      "WORD: omg its already o\n",
      "SWNt: omg its POS-1 o\n",
      "POS+POS_word: VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o\n",
      "----------------------------------------------------------------\n",
      "WORD: omgaga im sooo im gunna cry been at this dentist since i was suposed just get a crown put on\n",
      "SWNt: omgaga im sooo im gunna cry POS-2 at this dentist since i was suposed POS-6 get a crown put on\n",
      "POS+POS_word: JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN RB VB DT NN NN IN JJ_omgaga NN_im NN_sooo NN_im NN_gunna NN_cry VBN_been IN_at DT_this NN_dentist IN_since NN_i VBD_was VBN_suposed RB_just VB_get DT_a NN_crown NN_put IN_on\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    tweet = preprocessed_dataset.loc[i]['SentimentText']\n",
    "    print('WORD: ' + tweet_to_feature_word(tweet))\n",
    "    print('SWNt: ' + tweet_to_feature_SWNt(tweet))\n",
    "    print('POS+POS_word: ' + tweet_to_feature_POS(tweet))\n",
    "    #print('Semantic: ' + tweet_to_feature_semantic(tweet))\n",
    "    print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD\n",
      "Processing 99989/99989Processing 11364/99989Processing 12648/99989Processing 32953/99989Processing 63347/99989Processing 64747/99989Processing 67747/99989Processing 73962/99989\n",
      "SWNt\n",
      "Processing 99989/99989\n",
      "SWNt\n",
      "Processing 99989/99989"
     ]
    }
   ],
   "source": [
    "# WORD\n",
    "features_train_file = open('dataset/word_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('WORD')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_word(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()\n",
    "\n",
    "# SWNt\n",
    "features_train_file = open('dataset/swnt_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('\\nSWNt')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_SWNt(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()\n",
    "\n",
    "# POS+POS_word\n",
    "features_train_file = open('dataset/pos_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('\\nSWNt')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_POS(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry been at this denti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                        is so sad for my apl friend\n",
       "1       2          0                      i missed the new moon trailer\n",
       "2       3          1                                  omg its already o\n",
       "3       4          0  omgaga im sooo im gunna cry been at this denti...\n",
       "4       5          0                i think mi bf is cheating on me t_t"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORD\n",
    "features_word_dataset = pd.read_csv('dataset/word_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('WORD')\n",
    "features_word_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWNt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>POS-2 so NEG-8 for my apl POS-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i NEG-2 the POS-4 moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its POS-1 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry POS-2 at this dent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf POS-2 POS-6 on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                    POS-2 so NEG-8 for my apl POS-1\n",
       "1       2          0                     i NEG-2 the POS-4 moon trailer\n",
       "2       3          1                                    omg its POS-1 o\n",
       "3       4          0  omgaga im sooo im gunna cry POS-2 at this dent...\n",
       "4       5          0                i think mi bf POS-2 POS-6 on me t_t"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SWNt\n",
    "features_swnt_dataset = pd.read_csv('dataset/swnt_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('SWNt')\n",
    "features_swnt_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS+POS_word\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NN VBP NN NN VBZ VBG IN PRP VB NN_i VBP_think ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0  VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN...\n",
       "1       2          0  NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_n...\n",
       "2       3          1      VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o\n",
       "3       4          0  JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN R...\n",
       "4       5          0  NN VBP NN NN VBZ VBG IN PRP VB NN_i VBP_think ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS+POS_word\n",
    "features_pos_dataset = pd.read_csv('dataset/pos_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('POS+POS_word')\n",
    "features_pos_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = {}\n",
    "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
    "    result['URLS'] = tweet.count('URL')\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
    "        'URL', '')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams\n",
    "\n",
    "def get_bigrams(tweet_words):\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in range(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "def get_bigram_freqdist(bigrams):\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "Processing 99981/99990Processing 39001/99990\n",
      "Calculating frequency distribution\n",
      "Saved uni-frequency distribution to dataset/word/word_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/word/word_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 56457, Negative: 43532\n",
      "User Mentions => Total: 88955, Avg: 0.8896, Max: 12\n",
      "URLs => Total: 4237, Avg: 0.0424, Max: 4\n",
      "Emojis => Total: 1184, Positive: 997, Negative: 187, Avg: 0.0118, Max: 5\n",
      "Words => Total: 1154343, Unique: 48477, Avg: 11.5446, Max: 93, Min: 0\n",
      "Bigrams => Total: 1054840, Unique: 381798, Avg: 10.5495\n",
      "\n",
      "\n",
      "swnt\n",
      "Processing 99990/99990rocessing 10297/99990Processing 18695/99990Processing 47471/99990Processing 76651/99990\n",
      "Calculating frequency distribution\n",
      "Saved uni-frequency distribution to dataset/swnt/swnt_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/swnt/swnt_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 112914, Negative: 87064\n",
      "User Mentions => Total: 177910, Avg: 1.7793, Max: 12\n",
      "URLs => Total: 8474, Avg: 0.0847, Max: 4\n",
      "Emojis => Total: 2368, Positive: 1994, Negative: 374, Avg: 0.0237, Max: 5\n",
      "Words => Total: 2308686, Unique: 48495, Avg: 23.0892, Max: 93, Min: 0\n",
      "Bigrams => Total: 2109680, Unique: 435434, Avg: 21.0989\n",
      "\n",
      "\n",
      "pos\n",
      "Processing 99990/99990\n",
      "Calculating frequency distribution\n",
      "Saved uni-frequency distribution to dataset/pos/pos_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/pos/pos_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 169371, Negative: 130596\n",
      "User Mentions => Total: 266865, Avg: 2.6689, Max: 12\n",
      "URLs => Total: 12711, Avg: 0.1271, Max: 4\n",
      "Emojis => Total: 3552, Positive: 2991, Negative: 561, Avg: 0.0355, Max: 5\n",
      "Words => Total: 4802094, Unique: 122946, Avg: 48.0257, Max: 204, Min: 0\n",
      "Bigrams => Total: 4503099, Unique: 889697, Avg: 45.0355\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE FREQUENCY DISTRIBUTIONS (for each features dataset)\n",
    "num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "all_words = []\n",
    "all_bigrams = []\n",
    "datasets = ['word', 'swnt', 'pos']\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    with open('dataset/' + d + '/' + d + '_features_train.csv', 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        num_tweets = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            t_id, if_pos, tweet = line.strip().split(',')\n",
    "            if_pos = int(if_pos)\n",
    "            if if_pos:\n",
    "                num_pos_tweets += 1\n",
    "            else:\n",
    "                num_neg_tweets += 1\n",
    "            result, words, bigrams = analyze_tweet(tweet)\n",
    "            num_mentions += result['MENTIONS']\n",
    "            max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "            num_pos_emojis += result['POS_EMOS']\n",
    "            num_neg_emojis += result['NEG_EMOS']\n",
    "            max_emojis = max(\n",
    "                max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "            num_urls += result['URLS']\n",
    "            max_urls = max(max_urls, result['URLS'])\n",
    "            num_words += result['WORDS']\n",
    "            min_words = min(min_words, result['WORDS'])\n",
    "            max_words = max(max_words, result['WORDS'])\n",
    "            all_words.extend(words)\n",
    "            num_bigrams += result['BIGRAMS']\n",
    "            all_bigrams.extend(bigrams)\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('Processing %d/%d' % (i+1, num_tweets))\n",
    "            sys.stdout.flush()\n",
    "    num_emojis = num_pos_emojis + num_neg_emojis\n",
    "    unique_words = list(set(all_words))\n",
    "    with open('dataset/' + d + '/' + d + '_features_train_unique.txt', 'w') as uwf:\n",
    "        uwf.write('\\n'.join(unique_words))\n",
    "    num_unique_words = len(unique_words)\n",
    "    num_unique_bigrams = len(set(all_bigrams))\n",
    "    print('\\nCalculating frequency distribution')\n",
    "    # Unigrams\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    pkl_file_name = 'dataset/' + d + '/' + d + '_features_train_freqdist.pkl'\n",
    "    with open(pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(freq_dist, pkl_file)\n",
    "    print('Saved uni-frequency distribution to %s' % pkl_file_name)\n",
    "    # Bigrams\n",
    "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "    bi_pkl_file_name = 'dataset/' + d + '/' + d + '_features_train_freqdistbi.pkl'\n",
    "    with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(bigram_freq_dist, pkl_file)\n",
    "    print('Saved bi-frequency distribution to %s' % bi_pkl_file_name)\n",
    "    print('\\n[Analysis Statistics]')\n",
    "    print('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))\n",
    "    print('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))\n",
    "    print('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))\n",
    "    print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
    "    print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
    "    print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def split_data(tweets, validation_split=0.1):\n",
    "    index = int((1 - validation_split) * len(tweets))\n",
    "    random.shuffle(tweets)\n",
    "    return tweets[:index], tweets[index:]\n",
    "\n",
    "def top_n_words(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
    "    return words\n",
    "\n",
    "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
    "    return bigrams\n",
    "\n",
    "def apply_tf_idf(X):\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(X)\n",
    "    return transformer\n",
    "\n",
    "\n",
    "class FeatureVectorGenerator():\n",
    "    \n",
    "    def __init__(self, freq_dist_file, bi_freq_dist_file, train_processed_file, use_bigrams=True):\n",
    "        self.FREQ_DIST_FILE = freq_dist_file\n",
    "        self.BI_FREQ_DIST_FILE = bi_freq_dist_file\n",
    "        self.TRAIN_PROCESSED_FILE = train_processed_file\n",
    "        self.USE_BIGRAMS = use_bigrams\n",
    "        self.UNIGRAM_SIZE = 15000\n",
    "        self.VOCAB_SIZE = self.UNIGRAM_SIZE\n",
    "        if use_bigrams:\n",
    "            self.BIGRAM_SIZE = 10000\n",
    "            self.VOCAB_SIZE = self.UNIGRAM_SIZE + self.BIGRAM_SIZE\n",
    "    \n",
    "    def get_feature_vector(self, tweet):\n",
    "        # get the unigrams and bigrams contained in the vocabolary\n",
    "        uni_feature_vector = []\n",
    "        bi_feature_vector = []\n",
    "        words = tweet.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            word = words[i]\n",
    "            next_word = words[i + 1]\n",
    "            if self.unigrams.get(word):\n",
    "                uni_feature_vector.append(word)\n",
    "            if self.USE_BIGRAMS:\n",
    "                if self.bigrams.get((word, next_word)):\n",
    "                    bi_feature_vector.append((word, next_word))\n",
    "        if len(words) >= 1:\n",
    "            if self.unigrams.get(words[-1]):\n",
    "                uni_feature_vector.append(words[-1])\n",
    "        return uni_feature_vector, bi_feature_vector\n",
    "    \n",
    "    def extract_features(self, tweets, test_file=True, feat_type='presence'):\n",
    "        # feat_type can be 'presence' for bag of words or frequency for tf-idf\n",
    "        features = lil_matrix((len(tweets), self.VOCAB_SIZE))\n",
    "        labels = np.zeros(len(tweets))\n",
    "        for j, tweet in enumerate(tweets):\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('Processing %d/%d' % (j+1, len(tweets)))\n",
    "            sys.stdout.flush()\n",
    "            if test_file:\n",
    "                tweet_words = tweet[1][0]\n",
    "                tweet_bigrams = tweet[1][1]\n",
    "            else:\n",
    "                tweet_words = tweet[2][0]\n",
    "                tweet_bigrams = tweet[2][1]\n",
    "                labels[j] = tweet[1]\n",
    "            if feat_type == 'presence':\n",
    "                tweet_words = set(tweet_words)\n",
    "                tweet_bigrams = set(tweet_bigrams)\n",
    "            for word in tweet_words:\n",
    "                idx = self.unigrams.get(word)\n",
    "                if idx:\n",
    "                    features[j, idx] += 1\n",
    "            if self.USE_BIGRAMS:\n",
    "                for bigram in tweet_bigrams:\n",
    "                    idx = self.bigrams.get(bigram)\n",
    "                    if idx:\n",
    "                        features[j, self.UNIGRAM_SIZE + idx] += 1\n",
    "        if feat_type == 'frequency':\n",
    "            if not test_file:\n",
    "                self.tfidf = apply_tf_idf(features)\n",
    "            features = self.tfidf.transform(features)\n",
    "        return features, labels\n",
    "    \n",
    "    def extract_features_single(self, tweet, feat_type='presence'):\n",
    "        features = lil_matrix((1, self.VOCAB_SIZE))\n",
    "        tweet_words = tweet[0]\n",
    "        tweet_bigrams = tweet[1]\n",
    "        if feat_type == 'presence':\n",
    "            tweet_words = set(tweet_words)\n",
    "            tweet_bigrams = set(tweet_bigrams)\n",
    "        for word in tweet_words:\n",
    "            idx = self.unigrams.get(word)\n",
    "            if idx:\n",
    "                features[0, idx] += 1\n",
    "        if self.USE_BIGRAMS:\n",
    "            for bigram in tweet_bigrams:\n",
    "                idx = self.bigrams.get(bigram)\n",
    "                if idx:\n",
    "                    features[0, self.UNIGRAM_SIZE + idx] += 1\n",
    "        if feat_type == 'frequency':\n",
    "            features = self.tfidf.transform(features)\n",
    "        return features\n",
    "\n",
    "    def process_tweets(self, csv_file, test_file=True):\n",
    "        tweets = []\n",
    "        print('Generating feature vectors')\n",
    "        with open(csv_file, 'r') as csv:\n",
    "            lines = csv.readlines()\n",
    "            total = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if i == 0:\n",
    "                    # ignore header\n",
    "                    continue\n",
    "                if test_file:\n",
    "                    tweet_id, tweet = line.split(',')\n",
    "                else:\n",
    "                    tweet_id, sentiment, tweet = line.split(',')\n",
    "                feature_vector = self.get_feature_vector(tweet)\n",
    "                if test_file:\n",
    "                    tweets.append((tweet_id, feature_vector))\n",
    "                else:\n",
    "                    tweets.append((tweet_id, int(sentiment), feature_vector))\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write('Processing %d/%d' % (i+1, total))\n",
    "                sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        return tweets\n",
    "    \n",
    "    def fit(self):\n",
    "        self.unigrams = top_n_words(self.FREQ_DIST_FILE, self.UNIGRAM_SIZE)\n",
    "        if self.USE_BIGRAMS:\n",
    "            self.bigrams = top_n_bigrams(self.BI_FREQ_DIST_FILE, self.BIGRAM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 99990/99990Processing 21990/99990Processing 22790/99990Processing 23590/99990Processing 24390/99990Processing 34989/99990Processing 36589/99990Processing 61179/99990Processing 71376/99990\n",
      "\n",
      "Extracting features & training batches\n",
      "Processing 9999/999990rocessing 3982/89990Processing 5982/89990Processing 7982/89990Processing 9382/89990Processing 32179/89990Processing 37378/89990Processing 45377/89990Processing 60176/89990Processing 62776/89990Processing 83175/89990Processing 7980/9999"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/word/word_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/word/word_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/word/word_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "word_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "word_feature_generator.fit()\n",
    "tweets = word_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "word_training_set_X, word_training_set_y = word_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "word_validation_set_X, word_validation_set_y = word_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('78779', 1, (['POS-2', 'it', 'was', 'a', 'super', 'sweet', 'movie', 'sounds', 'POS-1', 'there', 'NEG-2', 'madly', 'in', 'POS-6', 'that', 'POS-2'], [('POS-2', 'it'), ('it', 'was'), ('was', 'a'), ('a', 'super'), ('sounds', 'POS-1'), ('POS-1', 'there'), ('there', 'NEG-2'), ('in', 'POS-6'), ('POS-6', 'that'), ('that', 'POS-2')]))\n",
      "  (0, 15247)\t0.592939059870466\n",
      "  (0, 15014)\t0.4326711641295425\n",
      "  (0, 252)\t0.5189303024374169\n",
      "  (0, 67)\t0.3864506252084344\n",
      "  (0, 2)\t0.206364218068287\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[0])\n",
    "print(word_validation_set_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWNt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 99990/99990rocessing 10164/99990Processing 12763/99990Processing 18761/99990Processing 19561/99990Processing 54757/99990Processing 55557/99990Processing 77956/99990Processing 78756/99990Processing 79556/99990Processing 80356/99990Processing 87156/99990\n",
      "\n",
      "Extracting features & training batches\n",
      "Processing 9369/999990Processing 10963/89990Processing 32361/89990Processing 35161/89990Processing 36518/89990Processing 39159/89990Processing 51356/89990Processing 75952/89990Processing 6755/9999"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/swnt/swnt_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/swnt/swnt_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/swnt/swnt_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "swnt_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "swnt_feature_generator.fit()\n",
    "tweets = swnt_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "swnt_training_set_X, swnt_training_set_y = swnt_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "swnt_validation_set_X, swnt_validation_set_y = swnt_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('78779', 1, (['POS-2', 'it', 'was', 'a', 'super', 'sweet', 'movie', 'sounds', 'POS-1', 'there', 'NEG-2', 'madly', 'in', 'POS-6', 'that', 'POS-2'], [('POS-2', 'it'), ('it', 'was'), ('was', 'a'), ('a', 'super'), ('sounds', 'POS-1'), ('POS-1', 'there'), ('there', 'NEG-2'), ('in', 'POS-6'), ('POS-6', 'that'), ('that', 'POS-2')]))\n",
      "  (0, 19856)\t0.3147555916391844\n",
      "  (0, 18103)\t0.27663036188226203\n",
      "  (0, 17671)\t0.2746992632891251\n",
      "  (0, 16485)\t0.25526056973342376\n",
      "  (0, 15567)\t0.22954685806630287\n",
      "  (0, 15295)\t0.2121164155183353\n",
      "  (0, 15176)\t0.20163291599171174\n",
      "  (0, 15128)\t0.19326239710717036\n",
      "  (0, 15101)\t0.21102998661198608\n",
      "  (0, 15015)\t0.17602099386521589\n",
      "  (0, 9748)\t0.3648116420498138\n",
      "  (0, 458)\t0.23277924768086494\n",
      "  (0, 298)\t0.2145579309778834\n",
      "  (0, 250)\t0.20803095995953877\n",
      "  (0, 180)\t0.19634791928677564\n",
      "  (0, 54)\t0.15266627690155346\n",
      "  (0, 24)\t0.1257952497531284\n",
      "  (0, 14)\t0.10915564731868259\n",
      "  (0, 13)\t0.08798804917420781\n",
      "  (0, 12)\t0.08758334429472282\n",
      "  (0, 11)\t0.10829652807431024\n",
      "  (0, 7)\t0.09762656305075727\n",
      "  (0, 6)\t0.1286333109336298\n",
      "  (0, 5)\t0.07224580915747346\n",
      "  (0, 4)\t0.08919723054027134\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[0])\n",
    "print(swnt_training_set_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS+POS_word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 99990/99990Processing 30540/99990Processing 31340/99990Processing 60937/99990Processing 61737/99990Processing 65137/99990Processing 65937/99990Processing 78136/99990Processing 79936/99990Processing 86736/99990Processing 87536/99990\n",
      "\n",
      "Extracting features & training batches\n",
      "Processing 9999/999990Processing 74136/89990Processing 84736/89990"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/pos/pos_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/pos/pos_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/pos/pos_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "pos_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "pos_feature_generator.fit()\n",
    "tweets = pos_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "pos_training_set_X, pos_training_set_y = pos_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "pos_validation_set_X, pos_validation_set_y = pos_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('60849', 0, (['NNP', 'IN', 'DT', 'NNS', 'WRB', 'JJ', 'VBD', 'PRP', 'RBS', 'JJ', 'JJ', 'VBZ', 'RB', 'JJ', 'RB', 'IN', 'DT', 'IN', 'IN_under', 'DT_the', 'NNS_circumstances', 'WRB_when', 'JJ_i', 'VBD_wrote', 'PRP_it', 'RBS_most', 'JJ_excellent', 'JJ_strange', 'VBZ_is', 'NN_way', 'RB_too', 'JJ_loud', 'RB_now', 'IN_for', 'DT_the', 'NN_drama', 'IN_of'], [('NNP', 'IN'), ('IN', 'DT'), ('DT', 'NNS'), ('NNS', 'WRB'), ('WRB', 'JJ'), ('JJ', 'VBD'), ('VBD', 'PRP'), ('RBS', 'JJ'), ('JJ', 'JJ'), ('JJ', 'NN'), ('NN', 'VBZ'), ('VBZ', 'NN'), ('NN', 'RB'), ('RB', 'JJ'), ('JJ', 'RB'), ('RB', 'IN'), ('IN', 'DT'), ('DT', 'NN'), ('NN', 'IN'), ('IN', 'NN'), ('IN_under', 'DT_the'), ('WRB_when', 'JJ_i'), ('NN_way', 'RB_too'), ('IN_for', 'DT_the')]))\n",
      "  (0, 24695)\t0.23558389104413344\n",
      "  (0, 20146)\t0.22064313044518338\n",
      "  (0, 17582)\t0.20192626129155694\n",
      "  (0, 16824)\t0.191883215348802\n",
      "  (0, 16212)\t0.18321104748387895\n",
      "  (0, 15328)\t0.14837120403298873\n",
      "  (0, 15243)\t0.14076848609038445\n",
      "  (0, 15207)\t0.13769726163874066\n",
      "  (0, 15162)\t0.1302786610668018\n",
      "  (0, 15158)\t0.12918131837227373\n",
      "  (0, 15076)\t0.11081369364807055\n",
      "  (0, 15061)\t0.10639897549784876\n",
      "  (0, 15054)\t0.10565208218921582\n",
      "  (0, 15040)\t0.10123158847835662\n",
      "  (0, 15035)\t0.09662566814458337\n",
      "  (0, 15021)\t0.0863115719019368\n",
      "  (0, 15011)\t0.07582701783991726\n",
      "  (0, 15009)\t0.07500286429913844\n",
      "  (0, 15006)\t0.06978665150186132\n",
      "  (0, 15004)\t0.11552289133229578\n",
      "  (0, 15003)\t0.06220868060791222\n",
      "  (0, 15002)\t0.05598835944652023\n",
      "  (0, 15001)\t0.05322994935138066\n",
      "  (0, 12641)\t0.26491491341684203\n",
      "  (0, 6090)\t0.23872849163671483\n",
      "  :\t:\n",
      "  (0, 3451)\t0.2194011275717038\n",
      "  (0, 3435)\t0.22107031151246634\n",
      "  (0, 1816)\t0.19660695511847506\n",
      "  (0, 1284)\t0.1830032779752289\n",
      "  (0, 1108)\t0.17924489754389847\n",
      "  (0, 350)\t0.14055659412799174\n",
      "  (0, 192)\t0.12106663948308227\n",
      "  (0, 137)\t0.10858200949253702\n",
      "  (0, 124)\t0.10639897549784876\n",
      "  (0, 77)\t0.0936795076731754\n",
      "  (0, 76)\t0.09292455877372535\n",
      "  (0, 63)\t0.08852592771556064\n",
      "  (0, 59)\t0.08500487581723003\n",
      "  (0, 55)\t0.08386246159691502\n",
      "  (0, 45)\t0.07706403900006045\n",
      "  (0, 22)\t0.1088240457360803\n",
      "  (0, 21)\t0.06119281303976759\n",
      "  (0, 17)\t0.05917687410818148\n",
      "  (0, 14)\t0.05252915526759578\n",
      "  (0, 9)\t0.041219920564539964\n",
      "  (0, 6)\t0.07486120698816288\n",
      "  (0, 5)\t0.044175383524839434\n",
      "  (0, 4)\t0.07301553851939323\n",
      "  (0, 2)\t0.08333934300978799\n",
      "  (0, 1)\t0.09454963976493196\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[3])\n",
    "print(pos_training_set_X[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess on test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hi', 'this', 'is', 'an', 'awesome', 'sample', 'tweet', 'to', 'try', 'how', 'testing', 'EMO_POS', 'car'], [('this', 'is'), ('is', 'an'), ('an', 'awesome'), ('tweet', 'to'), ('to', 'try')])\n",
      "  (0, 24259)\t0.3376941068180862\n",
      "  (0, 16125)\t0.29697791202285495\n",
      "  (0, 15769)\t0.2806029373441025\n",
      "  (0, 15723)\t0.27804595957867934\n",
      "  (0, 15095)\t0.22989261228624\n",
      "  (0, 5461)\t0.3270321841162721\n",
      "  (0, 2179)\t0.30629199609262525\n",
      "  (0, 411)\t0.24458434805598597\n",
      "  (0, 204)\t0.21647210763558936\n",
      "  (0, 201)\t0.21509077466989512\n",
      "  (0, 195)\t0.21790692466395373\n",
      "  (0, 157)\t0.20810836993518436\n",
      "  (0, 131)\t0.19716673664030696\n",
      "  (0, 92)\t0.17971289177657207\n",
      "  (0, 54)\t0.1687908874624308\n",
      "  (0, 34)\t0.15150167112019972\n",
      "  (0, 9)\t0.11670681571298516\n",
      "  (0, 2)\t0.09016934357250361\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hi, this is an awesome sample tweet to try how testing wooooorks! :) car'\n",
    "prep = preprocess_tweet(sentence)\n",
    "sentence_prep = word_feature_generator.get_feature_vector(prep)\n",
    "print(sentence_prep)\n",
    "print(word_feature_generator.extract_features_single(sentence_prep, feat_type='frequency'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the pipeline from tweet to features vector, train machine learning algorithms and see what happen. All the examples with \"word\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(val_set_labels, predictions):\n",
    "    print('Calculate accuracy')\n",
    "    acc = np.sum(predictions == val_set_labels) / len(val_set_labels) \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7731773177317732\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "clf_svm.fit(word_training_set_X, word_training_set_y)\n",
    "svm_prediction = clf_svm.predict(word_validation_set_X)\n",
    "svm_acc = calculate_accuracy(word_validation_set_y, svm_prediction)\n",
    "print('Accuracy: ' + str(svm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7724772477247724\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_logreg = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "clf_logreg.fit(word_training_set_X, word_training_set_y)\n",
    "logreg_prediction = clf_logreg.predict(word_validation_set_X)\n",
    "logreg_acc = calculate_accuracy(word_validation_set_y, logreg_prediction)\n",
    "print('Accuracy: ' + str(logreg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7661766176617661\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_nabay = MultinomialNB()\n",
    "clf_nabay.partial_fit(word_training_set_X, word_training_set_y, classes=[0, 1])\n",
    "nabay_prediction = clf_nabay.predict(word_validation_set_X)\n",
    "nabay_acc = calculate_accuracy(word_validation_set_y, nabay_prediction)\n",
    "print('Accuracy: ' + str(nabay_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-4110a2f9521d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7496749674967497\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rndfrs = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "clf_rndfrs.fit(word_training_set_X, word_training_set_y)\n",
    "rndfrs_prediction = clf_rndfrs.predict(word_validation_set_X)\n",
    "rndfrs_acc = calculate_accuracy(word_validation_set_y, rndfrs_prediction)\n",
    "print('Accuracy: ' + str(rndfrs_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ensemble:\n",
    "    \"\"\"\n",
    "                 | word | swnt | pos\n",
    "    ___________________________________\n",
    "    SVM          |      |      |\n",
    "    ___________________________________\n",
    "    logistic_reg |      |      |\n",
    "    ___________________________________\n",
    "    naive_bayes  |      |      |\n",
    "    ___________________________________\n",
    "    rnd_forest   |      |      |\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.matrix = []\n",
    "    \n",
    "    def fit(self, word_features, word_labels, swnt_features, swnt_labels, pos_features, pos_labels):\n",
    "        # SVM\n",
    "        clf_svm1 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        clf_svm1.fit(word_features, word_labels)\n",
    "        clf_svm2 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        clf_svm2.fit(swnt_features, swnt_labels)\n",
    "        clf_svm3 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        clf_svm3.fit(swnt_features, pos_labels)\n",
    "        self.matrix.append((clf_svm1, clf_svm2, clf_svm3))\n",
    "        # Logistic regression\n",
    "        clf_logreg1 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        clf_logreg1.fit(word_features, word_labels)\n",
    "        clf_logreg2 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        clf_logreg2.fit(swnt_features, swnt_labels)\n",
    "        clf_logreg3 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        clf_logreg3.fit(pos_features, pos_labels)\n",
    "        self.matrix.append((clf_logreg1, clf_logreg2, clf_logreg3))\n",
    "        # Naive Bayes\n",
    "        clf_nabay1 = MultinomialNB()\n",
    "        clf_nabay1.partial_fit(word_features, word_labels, classes=[0, 1])\n",
    "        clf_nabay2 = MultinomialNB()\n",
    "        clf_nabay2.partial_fit(swnt_features, swnt_labels, classes=[0, 1])\n",
    "        clf_nabay3 = MultinomialNB()\n",
    "        clf_nabay3.partial_fit(pos_features, pos_labels, classes=[0, 1])\n",
    "        self.matrix.append((clf_nabay1, clf_nabay2, clf_nabay3))\n",
    "        # Random Forest\n",
    "        clf_rndfrs1 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        clf_rndfrs1.fit(word_features, word_labels)\n",
    "        clf_rndfrs2 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        clf_rndfrs2.fit(swnt_features, swnt_labels)\n",
    "        clf_rndfrs3 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        clf_rndfrs3.fit(pos_features, pos_labels)\n",
    "        self.matrix.append((clf_rndfrs1, clf_rndfrs2, clf_rndfrs3))\n",
    "        return\n",
    "    \n",
    "    def predict(self, features):\n",
    "        return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = []\n",
    "a.append((clf_svm, clf_logreg))\n",
    "a.append((clf_nabay, clf_rndfrs))\n",
    "a[1][0].predict(word_validation_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = ensemble()\n",
    "e.fit(word_validation_set_X, word_validation_set_y, swnt_validation_set_X, swnt_validation_set_y, pos_validation_set_X, pos_validation_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_validation_set_y[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
