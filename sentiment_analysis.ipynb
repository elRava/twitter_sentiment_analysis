{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data given is in the form of a comma-separated values files with tweets and their corresponding sentiments. The training dataset is a csv file of type tweet_id,sentiment,tweet where the\n",
    "tweet_id is a unique integer identifying the tweet, sentiment is either 1 (positive) or 0 (negative), and tweet is the tweet enclosed in \"\". Similarly, the test dataset is a csv file of type\n",
    "tweet_id,tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import dill\n",
    "import sys\n",
    "import nltk\n",
    "sys.path.insert(0, 'src')\n",
    "sys.path.insert(1, 'airline')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save or restore notebook session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "dill.dump_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dill' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-69ab30d7a99a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdill\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'notebook_env.db'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dill' is not defined"
     ]
    }
   ],
   "source": [
    "dill.load_session('notebook_env.db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my APL frie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>I missed the New Moon trail...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already 7:30 :O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>.. Omgaga. Im sooo  im gunna CRy. I'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me!!!   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                       is so sad for my APL frie...\n",
       "1       2          0                     I missed the New Moon trail...\n",
       "2       3          1                            omg its already 7:30 :O\n",
       "3       4          0            .. Omgaga. Im sooo  im gunna CRy. I'...\n",
       "4       5          0           i think mi bf is cheating on me!!!   ..."
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = pd.read_csv('dataset/train.csv', encoding = \"ISO-8859-1\")\n",
    "dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "def handle_emojis(tweet):\n",
    "    # Smile -- :), : ), :-), (:, ( :, (-:, :')\n",
    "    tweet = re.sub(r'(:\\s?\\)|:-\\)|\\(\\s?:|\\(-:|:\\'\\))', ' EMO_POS ', tweet)\n",
    "    # Laugh -- :D, : D, :-D, xD, x-D, XD, X-D\n",
    "    tweet = re.sub(r'(:\\s?D|:-D|x-?D|X-?D)', ' EMO_POS ', tweet)\n",
    "    # Love -- <3, :*\n",
    "    tweet = re.sub(r'(<3|:\\*)', ' EMO_POS ', tweet)\n",
    "    # Wink -- ;-), ;), ;-D, ;D, (;,  (-;\n",
    "    tweet = re.sub(r'(;-?\\)|;-?D|\\(-?;)', ' EMO_POS ', tweet)\n",
    "    # Sad -- :-(, : (, :(, ):, )-:\n",
    "    tweet = re.sub(r'(:\\s?\\(|:-\\(|\\)\\s?:|\\)-:)', ' EMO_NEG ', tweet)\n",
    "    # Cry -- :,(, :'(, :\"(\n",
    "    tweet = re.sub(r'(:,\\(|:\\'\\(|:\"\\()', ' EMO_NEG ', tweet)\n",
    "    return tweet\n",
    "\n",
    "def preprocess_word(word):\n",
    "    # remove punctation\n",
    "    word = word.strip('\\'\"?!,.():;*')\n",
    "    # more than 3 letter repetition removed\n",
    "    word = re.sub(r'(.)\\1\\1+', r'\\1\\1\\1', word)\n",
    "    # remove - & '\n",
    "    word = word.strip('-&\\'')\n",
    "    return word\n",
    "\n",
    "def is_valid_word(word):\n",
    "    # Check if word begins with an alphabet\n",
    "    return (re.search(r'^[a-zA-Z][a-z0-9A-Z\\._]*$', word) is not None)\n",
    "\n",
    "def preprocess_tweet(tweet, use_stemmer=False, use_lemmatizer=False):\n",
    "    # convert tweet to lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # replace urls with 'URL'\n",
    "    tweet = re.sub(r'((www.[\\S]+)|(https?://.[\\S]+))', 'URL', tweet)\n",
    "    # replace user mentions @user with 'USER_MENTION'\n",
    "    tweet = re.sub(r'@[\\S]+', 'USER_MENTION', tweet)\n",
    "    # replace #hashtag with hastag\n",
    "    tweet = re.sub(r'#(\\S+)', r' \\1', tweet)\n",
    "    # remove retweet RT\n",
    "    tweet = re.sub(r'\\brt\\b', '', tweet)\n",
    "    # replace 2+ dots with space\n",
    "    tweet = re.sub(r'\\.{2,}', ' ', tweet)\n",
    "    # remove space, \" and ' \n",
    "    tweet.strip('\" \\'')\n",
    "    # handle emojis. Use only EMO_POS and EMO_NEG\n",
    "    tweet = handle_emojis(tweet)\n",
    "    # replace multiple spaces with only one space\n",
    "    tweet = re.sub(r'\\s+', ' ', tweet)\n",
    "    # preprocess words\n",
    "    words = tweet.split()\n",
    "\n",
    "    processed_words = []\n",
    "    porter_stemmer = PorterStemmer()\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        word = preprocess_word(word)\n",
    "        if is_valid_word(word):\n",
    "            if use_stemmer:\n",
    "                # use stemmer\n",
    "                word = str(porter_stemmer.stem(word))\n",
    "            elif use_lemmatizer:\n",
    "                word = str(wordnet_lemmatizer.lemmatize(word))\n",
    "            processed_words.append(word)\n",
    "    return ' '.join(processed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL:                      is so sad for my APL friend.............\n",
      "STEMMER: is so sad for my apl friend\n",
      "LEMMATIZER: is so sad for my apl friend\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:                    I missed the New Moon trailer...\n",
      "STEMMER: i miss the new moon trailer\n",
      "LEMMATIZER: i missed the new moon trailer\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:               omg its already 7:30 :O\n",
      "STEMMER: omg it alreadi o\n",
      "LEMMATIZER: omg it already o\n",
      "----------------------------------------------------------------\n",
      "ORIGINAL:           .. Omgaga. Im sooo  im gunna CRy. I've been at this dentist since 11.. I was suposed 2 just get a crown put on (30mins)...\n",
      "STEMMER: omgaga im sooo im gunna cri been at thi dentist sinc i wa supos just get a crown put on\n",
      "LEMMATIZER: omgaga im sooo im gunna cry been at this dentist since i wa suposed just get a crown put on\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "#tweet = \"Hi, how is going on? :) CIAO COME vaaaaa? :) :( https://tartarus.org/martin/PorterStemmer/  bella @ivocerti    #sentimentanalysis\"\n",
    "for i in range(4):\n",
    "    tweet = dataset.loc[i]['SentimentText']\n",
    "    print('ORIGINAL: ' + tweet)\n",
    "    print('STEMMER: ' + preprocess_tweet(tweet, use_stemmer=True))\n",
    "    print('LEMMATIZER: ' + preprocess_tweet(tweet, use_lemmatizer=True))\n",
    "    print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing 99989/99989"
     ]
    }
   ],
   "source": [
    "preprocessed_train_file = open('dataset/preprocessed_train.csv', 'w')\n",
    "preprocessed_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "for index, row in dataset.iterrows():\n",
    "    res = preprocess_tweet(row['SentimentText'])\n",
    "    preprocessed_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + preprocess_tweet(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry been at this denti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                        is so sad for my apl friend\n",
       "1       2          0                      i missed the new moon trailer\n",
       "2       3          1                                  omg its already o\n",
       "3       4          0  omgaga im sooo im gunna cry been at this denti...\n",
       "4       5          0                i think mi bf is cheating on me t_t"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessed_dataset = pd.read_csv('dataset/preprocessed_train.csv', encoding = \"ISO-8859-1\")\n",
    "preprocessed_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature parameters (Word, SentiWordNet, POS + POS_Word, Semantic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "\n",
    "def word_to_SWNt(word):\n",
    "    # take the first result from sentiwordnet (if there is)\n",
    "    list_sent = list(swn.senti_synsets(word))\n",
    "    if len(list_sent) > 0:\n",
    "        sent = list_sent[0]\n",
    "        pos = round(sent.pos_score()*10)\n",
    "        neg = round(sent.neg_score()*10)\n",
    "        obj = round(sent.obj_score()*10)\n",
    "        # return the word if the word itself is mostly object\n",
    "        # otherwise return POS-X or NEG-X if the word is \n",
    "        # mostly positive or negative and X is the rounded score*10 (from 0 to 10)\n",
    "        if pos > neg:\n",
    "            return 'POS-' + str(pos)\n",
    "        elif neg > pos:\n",
    "            return 'NEG-' + str(neg)\n",
    "    return word\n",
    "\n",
    "def tweet_to_feature_word(tweet):\n",
    "    # just return the tweet as it is\n",
    "    return str(tweet)\n",
    "\n",
    "def tweet_to_feature_SWNt(tweet):\n",
    "    words = str(tweet).split()\n",
    "    processed_words = []\n",
    "    for word in words:\n",
    "        processed_words.append(word_to_SWNt(word))\n",
    "    return str(' '.join(processed_words))\n",
    "\n",
    "def tweet_to_feature_POS(tweet):\n",
    "    # the tweet is composed by the POS (parts of speech) of the words\n",
    "    # first just the list of pos, then the list of the pos and the words\n",
    "    # in the format pos_word\n",
    "    words = str(tweet).split()\n",
    "    words_POS = nltk.pos_tag(words)\n",
    "    processed_words = []\n",
    "    pos = []\n",
    "    for word in words_POS:\n",
    "        pos.append(word[1])\n",
    "    processed_words = pos\n",
    "    size = len(processed_words)\n",
    "    for i in range(size):\n",
    "        processed_words.append(processed_words[i] + '_' + words[i])\n",
    "    return str(' '.join(processed_words))\n",
    "\n",
    "def tweet_to_feature_semantic(tweet):\n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD: is so sad for my apl friend\n",
      "SWNt: POS-2 so NEG-8 for my apl POS-1\n",
      "POS+POS_word: VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN_for PRP$_my NN_apl NN_friend\n",
      "----------------------------------------------------------------\n",
      "WORD: i missed the new moon trailer\n",
      "SWNt: i NEG-2 the POS-4 moon trailer\n",
      "POS+POS_word: NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_new NN_moon NN_trailer\n",
      "----------------------------------------------------------------\n",
      "WORD: omg its already o\n",
      "SWNt: omg its POS-1 o\n",
      "POS+POS_word: VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o\n",
      "----------------------------------------------------------------\n",
      "WORD: omgaga im sooo im gunna cry been at this dentist since i was suposed just get a crown put on\n",
      "SWNt: omgaga im sooo im gunna cry POS-2 at this dentist since i was suposed POS-6 get a crown put on\n",
      "POS+POS_word: JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN RB VB DT NN NN IN JJ_omgaga NN_im NN_sooo NN_im NN_gunna NN_cry VBN_been IN_at DT_this NN_dentist IN_since NN_i VBD_was VBN_suposed RB_just VB_get DT_a NN_crown NN_put IN_on\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(4):\n",
    "    tweet = preprocessed_dataset.loc[i]['SentimentText']\n",
    "    print('WORD: ' + tweet_to_feature_word(tweet))\n",
    "    print('SWNt: ' + tweet_to_feature_SWNt(tweet))\n",
    "    print('POS+POS_word: ' + tweet_to_feature_POS(tweet))\n",
    "    #print('Semantic: ' + tweet_to_feature_semantic(tweet))\n",
    "    print('----------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the whole datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD\n",
      "Processing 99989/99989Processing 11364/99989Processing 12648/99989Processing 32953/99989Processing 63347/99989Processing 64747/99989Processing 67747/99989Processing 73962/99989\n",
      "SWNt\n",
      "Processing 99989/99989\n",
      "SWNt\n",
      "Processing 99989/99989"
     ]
    }
   ],
   "source": [
    "# WORD\n",
    "features_train_file = open('dataset/word_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('WORD')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_word(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()\n",
    "\n",
    "# SWNt\n",
    "features_train_file = open('dataset/swnt_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('\\nSWNt')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_SWNt(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()\n",
    "\n",
    "# POS+POS_word\n",
    "features_train_file = open('dataset/pos_features_train.csv', 'w')\n",
    "features_train_file.write('ItemID,Sentiment,SentimentText\\n')\n",
    "print('\\nSWNt')\n",
    "for index, row in preprocessed_dataset.iterrows():\n",
    "    features_train_file.write(str(index+1) + ',' + str(row['Sentiment']) + ',' + tweet_to_feature_POS(row['SentimentText']) + '\\n')\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Processing ' + str(index+1) + '/' + str(preprocessed_dataset.shape[0]))\n",
    "    sys.stdout.flush()\n",
    "preprocessed_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORD\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>is so sad for my apl friend</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i missed the new moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its already o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry been at this denti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf is cheating on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                        is so sad for my apl friend\n",
       "1       2          0                      i missed the new moon trailer\n",
       "2       3          1                                  omg its already o\n",
       "3       4          0  omgaga im sooo im gunna cry been at this denti...\n",
       "4       5          0                i think mi bf is cheating on me t_t"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WORD\n",
    "features_word_dataset = pd.read_csv('dataset/word_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('WORD')\n",
    "features_word_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SWNt\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>POS-2 so NEG-8 for my apl POS-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>i NEG-2 the POS-4 moon trailer</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>omg its POS-1 o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>omgaga im sooo im gunna cry POS-2 at this dent...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>i think mi bf POS-2 POS-6 on me t_t</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0                    POS-2 so NEG-8 for my apl POS-1\n",
       "1       2          0                     i NEG-2 the POS-4 moon trailer\n",
       "2       3          1                                    omg its POS-1 o\n",
       "3       4          0  omgaga im sooo im gunna cry POS-2 at this dent...\n",
       "4       5          0                i think mi bf POS-2 POS-6 on me t_t"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# SWNt\n",
    "features_swnt_dataset = pd.read_csv('dataset/swnt_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('SWNt')\n",
    "features_swnt_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "POS+POS_word\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ItemID</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>SentimentText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_n...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN R...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>NN VBP NN NN VBZ VBG IN PRP VB NN_i VBP_think ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ItemID  Sentiment                                      SentimentText\n",
       "0       1          0  VBZ RB JJ IN PRP$ NN NN VBZ_is RB_so JJ_sad IN...\n",
       "1       2          0  NN VBD DT JJ NN NN NN_i VBD_missed DT_the JJ_n...\n",
       "2       3          1      VB PRP$ RB VB VB_omg PRP$_its RB_already VB_o\n",
       "3       4          0  JJ NN NN NN NN NN VBN IN DT NN IN NN VBD VBN R...\n",
       "4       5          0  NN VBP NN NN VBZ VBG IN PRP VB NN_i VBP_think ..."
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# POS+POS_word\n",
    "features_pos_dataset = pd.read_csv('dataset/pos_features_train.csv', encoding = \"ISO-8859-1\")\n",
    "print('POS+POS_word')\n",
    "features_pos_dataset.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import FreqDist\n",
    "import pickle\n",
    "from collections import Counter\n",
    "\n",
    "def analyze_tweet(tweet):\n",
    "    result = {}\n",
    "    result['MENTIONS'] = tweet.count('USER_MENTION')\n",
    "    result['URLS'] = tweet.count('URL')\n",
    "    result['POS_EMOS'] = tweet.count('EMO_POS')\n",
    "    result['NEG_EMOS'] = tweet.count('EMO_NEG')\n",
    "    tweet = tweet.replace('USER_MENTION', '').replace(\n",
    "        'URL', '')\n",
    "    words = tweet.split()\n",
    "    result['WORDS'] = len(words)\n",
    "    bigrams = get_bigrams(words)\n",
    "    result['BIGRAMS'] = len(bigrams)\n",
    "    return result, words, bigrams\n",
    "\n",
    "def get_bigrams(tweet_words):\n",
    "    bigrams = []\n",
    "    num_words = len(tweet_words)\n",
    "    for i in range(num_words - 1):\n",
    "        bigrams.append((tweet_words[i], tweet_words[i + 1]))\n",
    "    return bigrams\n",
    "\n",
    "def get_bigram_freqdist(bigrams):\n",
    "    freq_dict = {}\n",
    "    for bigram in bigrams:\n",
    "        if freq_dict.get(bigram):\n",
    "            freq_dict[bigram] += 1\n",
    "        else:\n",
    "            freq_dict[bigram] = 1\n",
    "    counter = Counter(freq_dict)\n",
    "    return counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word\n",
      "Processing 99784/99990Processing 50664/99990Saved uni-frequency distribution to dataset/word/word_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/word/word_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 56457, Negative: 43532\n",
      "User Mentions => Total: 88955, Avg: 0.8896, Max: 12\n",
      "URLs => Total: 4237, Avg: 0.0424, Max: 4\n",
      "Emojis => Total: 1184, Positive: 997, Negative: 187, Avg: 0.0118, Max: 5\n",
      "Words => Total: 1154343, Unique: 48477, Avg: 11.5446, Max: 93, Min: 0\n",
      "Bigrams => Total: 1054840, Unique: 381798, Avg: 10.5495\n",
      "\n",
      "\n",
      "swnt\n",
      "Processing 96715/99990Saved uni-frequency distribution to dataset/swnt/swnt_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/swnt/swnt_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 112914, Negative: 87064\n",
      "User Mentions => Total: 177910, Avg: 1.7793, Max: 12\n",
      "URLs => Total: 8474, Avg: 0.0847, Max: 4\n",
      "Emojis => Total: 2368, Positive: 1994, Negative: 374, Avg: 0.0237, Max: 5\n",
      "Words => Total: 2308686, Unique: 48495, Avg: 23.0892, Max: 93, Min: 0\n",
      "Bigrams => Total: 2109680, Unique: 435434, Avg: 21.0989\n",
      "\n",
      "\n",
      "pos\n",
      "Processing 99990/99990\n",
      "Calculating frequency distribution\n",
      "Saved uni-frequency distribution to dataset/pos/pos_features_train_freqdist.pkl\n",
      "Saved bi-frequency distribution to dataset/pos/pos_features_train_freqdistbi.pkl\n",
      "\n",
      "[Analysis Statistics]\n",
      "Tweets => Total: 99990, Positive: 169371, Negative: 130596\n",
      "User Mentions => Total: 266865, Avg: 2.6689, Max: 12\n",
      "URLs => Total: 12711, Avg: 0.1271, Max: 4\n",
      "Emojis => Total: 3552, Positive: 2991, Negative: 561, Avg: 0.0355, Max: 5\n",
      "Words => Total: 4802094, Unique: 122946, Avg: 48.0257, Max: 204, Min: 0\n",
      "Bigrams => Total: 4503099, Unique: 889697, Avg: 45.0355\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# CALCULATE FREQUENCY DISTRIBUTIONS (for each features dataset)\n",
    "num_tweets, num_pos_tweets, num_neg_tweets = 0, 0, 0\n",
    "num_mentions, max_mentions = 0, 0\n",
    "num_emojis, num_pos_emojis, num_neg_emojis, max_emojis = 0, 0, 0, 0\n",
    "num_urls, max_urls = 0, 0\n",
    "num_words, num_unique_words, min_words, max_words = 0, 0, 1e6, 0\n",
    "num_bigrams, num_unique_bigrams = 0, 0\n",
    "all_words = []\n",
    "all_bigrams = []\n",
    "datasets = ['word', 'swnt', 'pos']\n",
    "for d in datasets:\n",
    "    print(d)\n",
    "    with open('dataset/' + d + '/' + d + '_features_train.csv', 'r') as csv:\n",
    "        lines = csv.readlines()\n",
    "        num_tweets = len(lines)\n",
    "        for i, line in enumerate(lines):\n",
    "            if i == 0:\n",
    "                continue\n",
    "            t_id, if_pos, tweet = line.strip().split(',')\n",
    "            if_pos = int(if_pos)\n",
    "            if if_pos:\n",
    "                num_pos_tweets += 1\n",
    "            else:\n",
    "                num_neg_tweets += 1\n",
    "            result, words, bigrams = analyze_tweet(tweet)\n",
    "            num_mentions += result['MENTIONS']\n",
    "            max_mentions = max(max_mentions, result['MENTIONS'])\n",
    "            num_pos_emojis += result['POS_EMOS']\n",
    "            num_neg_emojis += result['NEG_EMOS']\n",
    "            max_emojis = max(\n",
    "                max_emojis, result['POS_EMOS'] + result['NEG_EMOS'])\n",
    "            num_urls += result['URLS']\n",
    "            max_urls = max(max_urls, result['URLS'])\n",
    "            num_words += result['WORDS']\n",
    "            min_words = min(min_words, result['WORDS'])\n",
    "            max_words = max(max_words, result['WORDS'])\n",
    "            all_words.extend(words)\n",
    "            num_bigrams += result['BIGRAMS']\n",
    "            all_bigrams.extend(bigrams)\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('Processing %d/%d' % (i+1, num_tweets))\n",
    "            sys.stdout.flush()\n",
    "    num_emojis = num_pos_emojis + num_neg_emojis\n",
    "    unique_words = list(set(all_words))\n",
    "    with open('dataset/' + d + '/' + d + '_features_train_unique.txt', 'w') as uwf:\n",
    "        uwf.write('\\n'.join(unique_words))\n",
    "    num_unique_words = len(unique_words)\n",
    "    num_unique_bigrams = len(set(all_bigrams))\n",
    "    print('\\nCalculating frequency distribution')\n",
    "    # Unigrams\n",
    "    freq_dist = FreqDist(all_words)\n",
    "    pkl_file_name = 'dataset/' + d + '/' + d + '_features_train_freqdist.pkl'\n",
    "    with open(pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(freq_dist, pkl_file)\n",
    "    print('Saved uni-frequency distribution to %s' % pkl_file_name)\n",
    "    # Bigrams\n",
    "    bigram_freq_dist = get_bigram_freqdist(all_bigrams)\n",
    "    bi_pkl_file_name = 'dataset/' + d + '/' + d + '_features_train_freqdistbi.pkl'\n",
    "    with open(bi_pkl_file_name, 'wb') as pkl_file:\n",
    "        pickle.dump(bigram_freq_dist, pkl_file)\n",
    "    print('Saved bi-frequency distribution to %s' % bi_pkl_file_name)\n",
    "    print('\\n[Analysis Statistics]')\n",
    "    print('Tweets => Total: %d, Positive: %d, Negative: %d' % (num_tweets, num_pos_tweets, num_neg_tweets))\n",
    "    print('User Mentions => Total: %d, Avg: %.4f, Max: %d' % (num_mentions, num_mentions / float(num_tweets), max_mentions))\n",
    "    print('URLs => Total: %d, Avg: %.4f, Max: %d' % (num_urls, num_urls / float(num_tweets), max_urls))\n",
    "    print('Emojis => Total: %d, Positive: %d, Negative: %d, Avg: %.4f, Max: %d' % (num_emojis, num_pos_emojis, num_neg_emojis, num_emojis / float(num_tweets), max_emojis))\n",
    "    print('Words => Total: %d, Unique: %d, Avg: %.4f, Max: %d, Min: %d' % (num_words, num_unique_words, num_words / float(num_tweets), max_words, min_words))\n",
    "    print('Bigrams => Total: %d, Unique: %d, Avg: %.4f' % (num_bigrams, num_unique_bigrams, num_bigrams / float(num_tweets)))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create features vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from scipy.sparse import lil_matrix\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "\n",
    "def split_data(tweets, validation_split=0.1):\n",
    "    random.seed(1118)\n",
    "    index = int((1 - validation_split) * len(tweets))\n",
    "    random.shuffle(tweets)\n",
    "    return tweets[:index], tweets[index:]\n",
    "\n",
    "def top_n_words(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    words = {p[0]: i + shift for i, p in enumerate(most_common)}\n",
    "    return words\n",
    "\n",
    "def top_n_bigrams(pkl_file_name, N, shift=0):\n",
    "    with open(pkl_file_name, 'rb') as pkl_file:\n",
    "        freq_dist = pickle.load(pkl_file)\n",
    "    most_common = freq_dist.most_common(N)\n",
    "    bigrams = {p[0]: i for i, p in enumerate(most_common)}\n",
    "    return bigrams\n",
    "\n",
    "def apply_tf_idf(X):\n",
    "    transformer = TfidfTransformer(smooth_idf=True, sublinear_tf=True, use_idf=True)\n",
    "    transformer.fit(X)\n",
    "    return transformer\n",
    "\n",
    "\n",
    "class FeatureVectorGenerator():\n",
    "    \n",
    "    def __init__(self, freq_dist_file, bi_freq_dist_file, train_processed_file, use_bigrams=True):\n",
    "        self.FREQ_DIST_FILE = freq_dist_file\n",
    "        self.BI_FREQ_DIST_FILE = bi_freq_dist_file\n",
    "        self.TRAIN_PROCESSED_FILE = train_processed_file\n",
    "        self.USE_BIGRAMS = use_bigrams\n",
    "        self.UNIGRAM_SIZE = 15000\n",
    "        self.VOCAB_SIZE = self.UNIGRAM_SIZE\n",
    "        if use_bigrams:\n",
    "            self.BIGRAM_SIZE = 10000\n",
    "            self.VOCAB_SIZE = self.UNIGRAM_SIZE + self.BIGRAM_SIZE\n",
    "    \n",
    "    def get_feature_vector(self, tweet):\n",
    "        # get the unigrams and bigrams contained in the vocabolary\n",
    "        uni_feature_vector = []\n",
    "        bi_feature_vector = []\n",
    "        words = tweet.split()\n",
    "        for i in range(len(words) - 1):\n",
    "            word = words[i]\n",
    "            next_word = words[i + 1]\n",
    "            if self.unigrams.get(word):\n",
    "                uni_feature_vector.append(word)\n",
    "            if self.USE_BIGRAMS:\n",
    "                if self.bigrams.get((word, next_word)):\n",
    "                    bi_feature_vector.append((word, next_word))\n",
    "        if len(words) >= 1:\n",
    "            if self.unigrams.get(words[-1]):\n",
    "                uni_feature_vector.append(words[-1])\n",
    "        return uni_feature_vector, bi_feature_vector\\\n",
    "    \n",
    "    def extract_features(self, tweets, test_file=True, feat_type='presence'):\n",
    "        # feat_type can be 'presence' for bag of words or frequency for tf-idf\n",
    "        features = lil_matrix((len(tweets), self.VOCAB_SIZE))\n",
    "        labels = np.zeros(len(tweets))\n",
    "        for j, tweet in enumerate(tweets):\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write('Processing %d/%d' % (j+1, len(tweets)))\n",
    "            sys.stdout.flush()\n",
    "            if test_file:\n",
    "                tweet_words = tweet[1][0]\n",
    "                tweet_bigrams = tweet[1][1]\n",
    "            else:\n",
    "                tweet_words = tweet[2][0]\n",
    "                tweet_bigrams = tweet[2][1]\n",
    "                labels[j] = tweet[1]\n",
    "            if feat_type == 'presence':\n",
    "                tweet_words = set(tweet_words)\n",
    "                tweet_bigrams = set(tweet_bigrams)\n",
    "            for word in tweet_words:\n",
    "                idx = self.unigrams.get(word)\n",
    "                if idx:\n",
    "                    features[j, idx] += 1\n",
    "            if self.USE_BIGRAMS:\n",
    "                for bigram in tweet_bigrams:\n",
    "                    idx = self.bigrams.get(bigram)\n",
    "                    if idx:\n",
    "                        features[j, self.UNIGRAM_SIZE + idx] += 1\n",
    "        if feat_type == 'frequency':\n",
    "            if not test_file:\n",
    "                self.tfidf = apply_tf_idf(features)\n",
    "            features = self.tfidf.transform(features)\n",
    "        return features, labels\n",
    "    \n",
    "    def extract_features_single(self, tweet, feat_type='presence'):\n",
    "        features = lil_matrix((1, self.VOCAB_SIZE))\n",
    "        tweet_words = tweet[0]\n",
    "        tweet_bigrams = tweet[1]\n",
    "        if feat_type == 'presence':\n",
    "            tweet_words = set(tweet_words)\n",
    "            tweet_bigrams = set(tweet_bigrams)\n",
    "        for word in tweet_words:\n",
    "            idx = self.unigrams.get(word)\n",
    "            if idx:\n",
    "                features[0, idx] += 1\n",
    "        if self.USE_BIGRAMS:\n",
    "            for bigram in tweet_bigrams:\n",
    "                idx = self.bigrams.get(bigram)\n",
    "                if idx:\n",
    "                    features[0, self.UNIGRAM_SIZE + idx] += 1\n",
    "        if feat_type == 'frequency':\n",
    "            features = self.tfidf.transform(features)\n",
    "        return features\n",
    "\n",
    "    def process_tweets(self, csv_file, test_file=True):\n",
    "        tweets = []\n",
    "        print('Generating feature vectors')\n",
    "        with open(csv_file, 'r') as csv:\n",
    "            lines = csv.readlines()\n",
    "            total = len(lines)\n",
    "            for i, line in enumerate(lines):\n",
    "                if i == 0:\n",
    "                    # ignore header\n",
    "                    continue\n",
    "                if test_file:\n",
    "                    tweet_id, tweet = line.split(',')\n",
    "                else:\n",
    "                    tweet_id, sentiment, tweet = line.split(',')\n",
    "                feature_vector = self.get_feature_vector(tweet)\n",
    "                if test_file:\n",
    "                    tweets.append((tweet_id, feature_vector))\n",
    "                else:\n",
    "                    tweets.append((tweet_id, int(sentiment), feature_vector))\n",
    "                sys.stdout.write('\\r')\n",
    "                sys.stdout.write('Processing %d/%d' % (i+1, total))\n",
    "                sys.stdout.flush()\n",
    "        print('\\n')\n",
    "        return tweets\n",
    "    \n",
    "    def fit(self):\n",
    "        self.unigrams = top_n_words(self.FREQ_DIST_FILE, self.UNIGRAM_SIZE)\n",
    "        if self.USE_BIGRAMS:\n",
    "            self.bigrams = top_n_bigrams(self.BI_FREQ_DIST_FILE, self.BIGRAM_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 99990/99990Processing 29526/99990Processing 33925/99990Processing 49918/99990Processing 90305/99990\n",
      "\n",
      "Extracting features & training batches\n",
      "Processing 9999/999990Processing 89068/89990"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/word/word_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/word/word_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/word/word_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "word_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "word_feature_generator.fit()\n",
    "tweets = word_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "word_training_set_X, word_training_set_y = word_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "word_validation_set_X, word_validation_set_y = word_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('84378', 0, (['wish', 'were', 'at', 'the', 'beach'], [('i', 'wish'), ('wish', 'i'), ('i', 'were'), ('were', 'at'), ('at', 'the'), ('the', 'beach')]))\n",
      "  (0, 20860)\t0.45624438783837346\n",
      "  (0, 16267)\t0.3824592958334005\n",
      "  (0, 15898)\t0.37465163017086883\n",
      "  (0, 15040)\t0.26960692354169286\n",
      "  (0, 15037)\t0.2702038648270284\n",
      "  (0, 15034)\t0.26684255364065795\n",
      "  (0, 658)\t0.34700312000734956\n",
      "  (0, 120)\t0.2470853353381329\n",
      "  (0, 112)\t0.24243535882803996\n",
      "  (0, 29)\t0.18780180279567585\n",
      "  (0, 1)\t0.1131034164249352\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[0])\n",
    "print(word_training_set_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SWNt model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 99862/99990Processing 40992/99990Processing 53382/99990\n",
      "\n",
      "Extracting features & training batches\n",
      "Processing 9999/999990Processing 34767/89990Processing 41519/89990"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/swnt/swnt_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/swnt/swnt_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/swnt/swnt_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "swnt_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "swnt_feature_generator.fit()\n",
    "tweets = swnt_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "swnt_training_set_X, swnt_training_set_y = swnt_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "swnt_validation_set_X, swnt_validation_set_y = swnt_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('84378', 0, (['POS-5', 'POS-2', 'at', 'the', 'beach'], [('i', 'POS-5'), ('POS-5', 'i'), ('i', 'POS-2'), ('POS-2', 'at'), ('at', 'the'), ('the', 'beach')]))\n",
      "  (0, 16851)\t0.43853622575927786\n",
      "  (0, 16017)\t0.4441484939367343\n",
      "  (0, 15057)\t0.2937529457503533\n",
      "  (0, 15052)\t0.2909081151635931\n",
      "  (0, 15033)\t0.31634112539074666\n",
      "  (0, 15017)\t0.2554330607283537\n",
      "  (0, 611)\t0.4113712599417268\n",
      "  (0, 32)\t0.2226385291110598\n",
      "  (0, 15)\t0.14748695554162722\n",
      "  (0, 6)\t0.12512761180078183\n",
      "  (0, 1)\t0.13408379416719343\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[0])\n",
    "print(swnt_training_set_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS+POS_word model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating feature vectors\n",
      "Processing 9164/999990Processing 74291/99990"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "FREQ_DIST_FILE = 'dataset/pos/pos_features_train_freqdist.pkl'\n",
    "BI_FREQ_DIST_FILE = 'dataset/pos/pos_features_train_freqdistbi.pkl'\n",
    "TRAIN_PROCESSED_FILE = 'dataset/pos/pos_features_train.csv'\n",
    "FEAT_TYPE = 'frequency'\n",
    "#TEST_PROCESSED_FILE = 'dataset/test-processed.csv'\n",
    "\n",
    "pos_feature_generator = FeatureVectorGenerator(FREQ_DIST_FILE, BI_FREQ_DIST_FILE, TRAIN_PROCESSED_FILE)\n",
    "pos_feature_generator.fit()\n",
    "tweets = pos_feature_generator.process_tweets(TRAIN_PROCESSED_FILE, test_file=False)\n",
    "train_tweets, val_tweets = split_data(tweets)\n",
    "\n",
    "print('Extracting features & training batches')\n",
    "pos_training_set_X, pos_training_set_y = pos_feature_generator.extract_features(train_tweets, test_file=False, feat_type=FEAT_TYPE)\n",
    "pos_validation_set_X, pos_validation_set_y = pos_feature_generator.extract_features(val_tweets, test_file=False, feat_type=FEAT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('84378', 0, (['JJ', 'VBP', 'NNS', 'VBD', 'IN', 'DT', 'JJ_i', 'VBP_wish', 'NNS_i', 'VBD_were', 'IN_at', 'DT_the', 'NN_beach'], [('JJ', 'VBP'), ('VBP', 'NNS'), ('NNS', 'VBD'), ('VBD', 'IN'), ('IN', 'DT'), ('DT', 'NN'), ('NN', 'JJ_i'), ('JJ_i', 'VBP_wish'), ('IN_at', 'DT_the'), ('DT_the', 'NN_beach')]))\n",
      "  (0, 19149)\t0.3578309102866506\n",
      "  (0, 18666)\t0.3556783871388087\n",
      "  (0, 17317)\t0.33213732860379186\n",
      "  (0, 15361)\t0.25332901676051495\n",
      "  (0, 15206)\t0.22984293470666342\n",
      "  (0, 15142)\t0.21189358496122349\n",
      "  (0, 15105)\t0.197060377692977\n",
      "  (0, 15043)\t0.1694604797492354\n",
      "  (0, 15004)\t0.11416514609807148\n",
      "  (0, 15002)\t0.09369969703889273\n",
      "  (0, 1800)\t0.3315213711236042\n",
      "  (0, 792)\t0.2805652080828856\n",
      "  (0, 346)\t0.23457235063580986\n",
      "  (0, 147)\t0.18612134162457872\n",
      "  (0, 121)\t0.17829107613828402\n",
      "  (0, 77)\t0.15669087500633502\n",
      "  (0, 22)\t0.10737559240183435\n",
      "  (0, 17)\t0.0988938248318331\n",
      "  (0, 14)\t0.08787082538150585\n",
      "  (0, 7)\t0.07689983442482862\n",
      "  (0, 6)\t0.07402091733507168\n",
      "  (0, 2)\t0.06650848940908176\n",
      "  (0, 1)\t0.06628965921262862\n"
     ]
    }
   ],
   "source": [
    "print(train_tweets[0])\n",
    "print(pos_training_set_X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### preprocess on test tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['hi', 'this', 'is', 'an', 'awesome', 'sample', 'tweet', 'to', 'try', 'how', 'testing', 'EMO_POS', 'car'], [('this', 'is'), ('is', 'an'), ('an', 'awesome'), ('tweet', 'to'), ('to', 'try')])\n",
      "  (0, 24259)\t0.3480045586192851\n",
      "  (0, 16125)\t0.2973134832617887\n",
      "  (0, 15769)\t0.28248729415910595\n",
      "  (0, 15723)\t0.27956046199395196\n",
      "  (0, 15095)\t0.22615956330715867\n",
      "  (0, 5461)\t0.32265902094053694\n",
      "  (0, 2179)\t0.31449958028172165\n",
      "  (0, 411)\t0.24231556737767498\n",
      "  (0, 204)\t0.2185262595172075\n",
      "  (0, 201)\t0.21047296082411993\n",
      "  (0, 195)\t0.20569671871002487\n",
      "  (0, 157)\t0.21090063682832816\n",
      "  (0, 131)\t0.19453016949482776\n",
      "  (0, 92)\t0.17817831127895933\n",
      "  (0, 54)\t0.166919353923811\n",
      "  (0, 34)\t0.14907699015634612\n",
      "  (0, 9)\t0.11581074529912529\n",
      "  (0, 2)\t0.08792113847315476\n"
     ]
    }
   ],
   "source": [
    "sentence = 'Hi, this is an awesome sample tweet to try how testing wooooorks! :) car'\n",
    "prep = preprocess_tweet(sentence)\n",
    "sentence_prep = word_feature_generator.get_feature_vector(prep)\n",
    "print(sentence_prep)\n",
    "print(word_feature_generator.extract_features_single(sentence_prep, feat_type='frequency'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that I have the pipeline from tweet to features vector, train machine learning algorithms and see what happen. All the examples with \"word\" dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(val_set_labels, predictions):\n",
    "    print('Calculate accuracy')\n",
    "    acc = np.sum(predictions == val_set_labels) / len(val_set_labels) \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7753775377537754\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "clf_svm = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "clf_svm.fit(word_training_set_X, word_training_set_y)\n",
    "svm_prediction = clf_svm.predict(word_validation_set_X)\n",
    "svm_acc = calculate_accuracy(word_validation_set_y, svm_prediction)\n",
    "print('Accuracy: ' + str(svm_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7746774677467747\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf_logreg = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "clf_logreg.fit(word_training_set_X, word_training_set_y)\n",
    "logreg_prediction = clf_logreg.predict(word_validation_set_X)\n",
    "logreg_acc = calculate_accuracy(word_validation_set_y, logreg_prediction)\n",
    "print('Accuracy: ' + str(logreg_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.7718771877187719\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "clf_nabay = MultinomialNB()\n",
    "clf_nabay.partial_fit(word_training_set_X, word_training_set_y, classes=[0, 1])\n",
    "nabay_prediction = clf_nabay.predict(word_validation_set_X)\n",
    "nabay_acc = calculate_accuracy(word_validation_set_y, nabay_prediction)\n",
    "print('Accuracy: ' + str(nabay_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-142-4110a2f9521d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(500, input_dim=VOCAB_SIZE, activation='sigmoid'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculate accuracy\n",
      "Accuracy: 0.753975397539754\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf_rndfrs = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "clf_rndfrs.fit(word_training_set_X, word_training_set_y)\n",
    "rndfrs_prediction = clf_rndfrs.predict(word_validation_set_X)\n",
    "rndfrs_acc = calculate_accuracy(word_validation_set_y, rndfrs_prediction)\n",
    "print('Accuracy: ' + str(rndfrs_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Ensemble:\n",
    "    \"\"\"\n",
    "                 | word | swnt | pos\n",
    "    ___________________________________\n",
    "    SVM          |      |      |\n",
    "    ___________________________________\n",
    "    logistic_reg |      |      |\n",
    "    ___________________________________\n",
    "    naive_bayes  |      |      |\n",
    "    ___________________________________\n",
    "    rnd_forest   |      |      |\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.matrix = []\n",
    "    \n",
    "    def fit(self, word_features, word_labels, swnt_features, swnt_labels, pos_features, pos_labels):\n",
    "        # SVM\n",
    "        clf_svm1 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 1/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_svm1.fit(word_features, word_labels)\n",
    "        clf_svm2 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 2/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_svm2.fit(swnt_features, swnt_labels)\n",
    "        clf_svm3 = svm.LinearSVC(C=0.1, max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 3/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_svm3.fit(pos_features, pos_labels)\n",
    "        self.matrix.append((clf_svm1, clf_svm2, clf_svm3))\n",
    "        # Logistic regression\n",
    "        clf_logreg1 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 4/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_logreg1.fit(word_features, word_labels)\n",
    "        clf_logreg2 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 5/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_logreg2.fit(swnt_features, swnt_labels)\n",
    "        clf_logreg3 = LogisticRegression(random_state=0, solver='lbfgs', max_iter=1000)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 6/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_logreg3.fit(pos_features, pos_labels)\n",
    "        self.matrix.append((clf_logreg1, clf_logreg2, clf_logreg3))\n",
    "        # Naive Bayes\n",
    "        clf_nabay1 = MultinomialNB()\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 7/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_nabay1.partial_fit(word_features, word_labels, classes=[0, 1])\n",
    "        clf_nabay2 = MultinomialNB()\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 8/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_nabay2.partial_fit(swnt_features, swnt_labels, classes=[0, 1])\n",
    "        clf_nabay3 = MultinomialNB()\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 9/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_nabay3.partial_fit(pos_features, pos_labels, classes=[0, 1])\n",
    "        self.matrix.append((clf_nabay1, clf_nabay2, clf_nabay3))\n",
    "        # Random Forest\n",
    "        clf_rndfrs1 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 10/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_rndfrs1.fit(word_features, word_labels)\n",
    "        clf_rndfrs2 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 11/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_rndfrs2.fit(swnt_features, swnt_labels)\n",
    "        clf_rndfrs3 = RandomForestClassifier(n_estimators=100, n_jobs=2, random_state=0)\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write('Fitting 12/12')\n",
    "        sys.stdout.flush()\n",
    "        clf_rndfrs3.fit(pos_features, pos_labels)\n",
    "        self.matrix.append((clf_rndfrs1, clf_rndfrs2, clf_rndfrs3))\n",
    "        return\n",
    "    \n",
    "    # pass one vector a time, this because word, swnt, pos are not ordered in the same way\n",
    "    def predict(self, word_features, swnt_features, pos_features):\n",
    "        features = (word_features, swnt_features, pos_features)\n",
    "        predictions = []\n",
    "        for i in range(len(m)):\n",
    "            for j in range(len(m[0])):\n",
    "                pred = m[i][j].predict(features[j])\n",
    "                predictions.append(pred[0])\n",
    "        return predictions\n",
    "    \n",
    "    def save_matrix(self, path):\n",
    "        with open(path, 'wb') as pkl_file:\n",
    "            pickle.dump(self.matrix, pkl_file)\n",
    "            \n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as matrix_file:\n",
    "            self.matrix = pickle.load(matrix_file)\n",
    "            \n",
    "#     def evaluate(self, word_features, swnt_features, pos_features, labels):\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 0, 1, 1])"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.append((clf_svm, clf_logreg))\n",
    "a.append((clf_nabay, clf_rndfrs))\n",
    "a[1][0].predict(word_validation_set_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = Ensemble()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 12/12"
     ]
    }
   ],
   "source": [
    "e.fit(word_training_set_X, word_training_set_y, swnt_training_set_X, swnt_training_set_y, pos_training_set_X, pos_training_set_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.save_matrix('dataset/matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.load('dataset/matrix.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LinearSVC(C=0.1, class_weight=None, dual=True, fit_intercept=True,\n",
      "     intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
      "     multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
      "     verbose=0)\n",
      "3\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "m = e.matrix\n",
    "print(m[0][0])\n",
    "print(len(m[0]))\n",
    "print(len(m))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if correctly shuffled\n",
    "for i in range(1000):\n",
    "    if word_validation_set_y[i] != swnt_validation_set_y[i] or swnt_validation_set_y[i] != pos_validation_set_y[i]:\n",
    "        print('ahia')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verify performance of ensembled model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting 9999/9999 - Correct 7751/9999 - Accuracy 0.775Correct: 7752 / 9999\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = word_validation_set_X.shape[0]\n",
    "final_predictions = []\n",
    "for i in range(total):\n",
    "    sys.stdout.write('\\r')\n",
    "    sys.stdout.write('Predicting %d/%d - Correct %d/%d - Accuracy %.3f' % (i+1, total, correct, total, correct/(i+1)))\n",
    "    sys.stdout.flush()\n",
    "    p = e.predict(word_validation_set_X[i], swnt_validation_set_X[i], pos_validation_set_X[i])\n",
    "    counts = np.bincount(p)\n",
    "    final_pred = np.argmax(counts)\n",
    "    final_predictions.append(final_pred)\n",
    "    if final_pred == word_validation_set_y[i]:\n",
    "        correct += 1\n",
    "print('Correct: ' + str(correct) + ' / ' + str(total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9999, 25000)\n"
     ]
    }
   ],
   "source": [
    "print(word_validation_set_X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
